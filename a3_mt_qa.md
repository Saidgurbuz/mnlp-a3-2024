
# Part 3 - Open-Answer Questions

#### Q1: How is BERTScore calculated? Read the first three paragraphs in Section 3 -- called "Token representation", "Similarity Measure", and "BERTScore" -- in [this paper](https://arxiv.org/pdf/1904.09675.pdf) and give a technical description of how the BERTScore precision/recall/f1 is calculated in ~6 sentences. You do not need to describe anything outside the scope of these specific paragraphs.

First of all, both reference sentence x and candidate sentence x^ are tokenized and contextual embeddings like BERT is used to represent each token which also considers the surrounding context. Then, the cosine similarity between each reference token embeddings x_i and candidate token embeddings x^_j are calculated as their inner products with their pre-normalized vectors (x_i^T x^_j). For each token in x and x^, greedy matching (for x_i max(x_i^T x^_j) and for x_^j vice versa) is performed to find the token with maximum cosine similarity (which is most similar) in the other sentence which is then used to calculate recall and precision respectively. More precisely, precision is calculated by dividing the total similarity score of x^ to matched tokens in x by the number of tokens in x^. Similarly, recall is calculated by dividing the total similarity score of x to matched tokens in x^ by the number of tokens in x. Finally, F1 score is computed by taking the harmonic mean of precision and recall, namely 2 * precision * recall / (precision + recall).

#### Q2: How is COMET trained and calculated? Read Section 2.4 -- "Translation Ranking Model" -- in [this paper](https://arxiv.org/pdf/2009.09025.pdf) and give a technical description in ~6 sentences.

For training, COMET uses a ranking model that takes the reference sentence, candidate sentence, better hypothesis and worse hypothesis as input (X : (s, h+, h-, r) ). These inputs are passed to pre-trained cross-lingual encoder and then pooling layer to get the sentence embeddings for each of them. Using these embeddings, the triplet margin loss is computed in relation to the s and reference r. The loss function optimizes the embedding space so that the distance between the anchors (s and r) and the worse hypothesis is greater by at least a margin epsilon than the distance between the anchors and the better hypothesis h+ (goal: d(anchors, h-) - d(anchors, h+) >= epsilon). During inference, the model uses the reference sentence, candidate sentence and hypothesis (only one hypothesis) as input (args: (s,h,r) ). The quality score assigned to h is the harmonic mean between the distance of hypothesis to the reference and the distance of hypothesis to the source (2 * d(h, r) * d(h, s) / (d(h, r) + d(h, s))). Finally, the resulting score is normalized to be between 0 and 1.

#### Q3: Given your understanding of BLEU, BERTScore and COMET, how would you interpret the Kendall's Tau correlation results? Which ones are the least and most correlated? What is your hypothesis regarding the reasons behind the lowest correlation for one metric and the highest correlation in the other?

I got the worst Kendall's Tau correlation for BLEUs (~0.150) and the best for COMET (0.289). The lowest correlation for BLEU is most probably due to the fact that BLEU is an n-gram based metric which does not take into account the semantic similarity between the reference and candidate sentences but directly checks for exact n-gram matches (when they give semantically same sentence with different wordings (for instance paraphrased version) then BLUE gives very bad results) which can lead to poor performance in capturing the true quality of the translations. On the other hand, the highest correlation for COMET (and second highest is BERTScore) is because they both use contextual embeddings (and COMET leverages a transformer based ranking model that is pre-trained on cross-lingual translation data) which allow them to better capture the semantic similarity between the reference and candidate sentences. This makes them more robust different wordings of the same sentence and hence more correlated with human judgement.


#### Q4: Assume you have a large set of story beginnings and you would like to evaluate how well a model completes the stories. What problem would you run into with BLEU and COMET? Would the same disadvantages apply to BERTScore and why? Give your justification. Answer in ~6 sentences.

For BLEU, the main problem is that, it is an n-gram based metric which does not take into account the semantic similarity between the reference and candidate stories and since story completion is very open-ended, it is very likely that the candidate story will not have the same n-grams as the reference story, so, it couldn't capture the quality of the story completions. For COMET, the problem is that, it is trained on translation data and it is not clear how well it can generalize to story completion tasks. However, the disadvantages of BLUE and COMET would not apply to BERTscore since it leverages contextual embeddings from pretrained models like BERT which can capture the semantic similarity between the reference and candidate stories even if they do not have the same n-grams. Moreover, since BERTScore is not specifically trained on translation data, and since it it calculating the pairwise similarity between the reference and candidate stories, it is more likely to be able to generalize to story completion tasks. The reason is that even story completions are very open-ended, since BERTScore use greedy matching to find the most similar tokens (contextual embeddings) in the other sentence, it can capture the semantic similarity even for creative completions of the story beginnings. However, BERTScore may still struggle with evaluating higher-level aspects of story quality which require more holistic understanding of the narrative structure.
